{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, bs4\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import trange\n",
    "\n",
    "pd.set_option(\"display.max_columns\",100)\n",
    "pd.set_option(\"display.max_rows\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptt_crawler(page, board='Gossiping'):\n",
    "    try:\n",
    "        url = f'https://www.ptt.cc/bbs/{board}/index{page}.html'\n",
    "        response = requests.get(url, cookies={'over18':'1'})\n",
    "\n",
    "        # 解析內容 (轉為string)\n",
    "        content = response.content.decode()\n",
    "        # 把string轉為hmtl node tree，回傳根節點\n",
    "        html = etree.HTML(content)\n",
    "\n",
    "        # 文章標題\n",
    "        title = html.xpath(\"//div[@class='r-ent']/div[@class='title']/a[@href]/text()\")\n",
    "        #push = html.xpath(\"//div[@class='nrec']/span/text()\")\n",
    "\n",
    "        # 文章url\n",
    "        title_url_o = html.xpath(\"//div[@class='r-ent']/div[@class='title']//@href\")\n",
    "        title_url=[]\n",
    "        for i in title_url_o:\n",
    "            title_url_i = f'https://www.ptt.cc{i}'\n",
    "            title_url.append(title_url_i)\n",
    "\n",
    "        # 各個文章內容\n",
    "        sub_response=[]\n",
    "        sub_content=[]\n",
    "        sub_html=[]\n",
    "        for sub_url in title_url:\n",
    "            sub_response.append(requests.get(sub_url, cookies={'over18':'1'}))\n",
    "        for s_r in sub_response:\n",
    "            sub_content.append(s_r.content.decode())\n",
    "        for s_c in sub_content:\n",
    "            sub_html.append(etree.HTML(s_c))\n",
    "\n",
    "        article_content_o=[]\n",
    "        article_content_o_null=[]\n",
    "        author_o=[]\n",
    "        date_o=[]\n",
    "        push_id=[]\n",
    "        push_content=[]\n",
    "        push_ip_time=[]\n",
    "        push_good=[]\n",
    "        push_bad=[]\n",
    "        for s_h in sub_html:\n",
    "            article_content_o.append(s_h.xpath(\"//div[@id='main-content']/text()\"))\n",
    "            article_content_o_null.append(s_h.xpath(\"(//div[@id='main-content']/text())[2]\"))\n",
    "            author_o.append(s_h.xpath(\"(//span[@class='article-meta-value'])[1]/text()\"))\n",
    "            date_o.append(s_h.xpath(\"(//span[@class='article-meta-value'])[4]/text()\"))\n",
    "            push_id.append(s_h.xpath(\"//div[@class='push']/span[@class='f3 hl push-userid']/text()\"))\n",
    "            push_content.append(s_h.xpath(\"//div[@class='push']/span[@class='f3 push-content']/text()\"))\n",
    "            push_ip_time.append(s_h.xpath(\"//div[@class='push']/span[@class='push-ipdatetime']/text()\"))\n",
    "            push_good.append(s_h.xpath(\"//span[@class='hl push-tag']/text()\"))\n",
    "            push_bad.append(s_h.xpath(\"//span[@class='f1 hl push-tag']/text()\"))\n",
    "\n",
    "\n",
    "        # 推文ID+推文內容+推文IP與時間\n",
    "        push_all_o = []\n",
    "\n",
    "        for i,j,k in zip(push_id,push_content,push_ip_time):\n",
    "            push_all_o.append([i,j,k])\n",
    "\n",
    "        # 文章推文的表\n",
    "        #push_c = {'推文ID':push_id, '推文內容':push_content, '推文IP與時間':push_ip_time, 'ALL':push_all_o}\n",
    "        #push_parse = pd.DataFrame(push_c) \n",
    "\n",
    "        # 去括號\n",
    "        article_content=[]\n",
    "        article_content_null=[]\n",
    "        author=[]\n",
    "        date=[]\n",
    "        push_all=[]\n",
    "        good_bad=[]\n",
    "        for i in range(len(title)):\n",
    "            try:\n",
    "                article_content_null.append(article_content_o_null[i][0])\n",
    "            except:\n",
    "                article_content_null.append('')\n",
    "            article_content.append(article_content_o[i][0])\n",
    "            author.append(author_o[i][0])\n",
    "            date.append(date_o[i][0])\n",
    "            push_all.append(push_all_o[i][0])\n",
    "            good_bad.append(len(push_good[i])-len(push_bad[i]))\n",
    "\n",
    "        article_content = [re.sub('\\n', '', article_content[i]) for i in range(len(article_content))]\n",
    "        article_content_null = [re.sub('\\n', '', article_content_null[i]) for i in range(len(article_content_null))]\n",
    "\n",
    "        # 時間分割\n",
    "        date = [re.split(' ', date[i]) for i in range(len(date))]\n",
    "        Day_of_week=[]\n",
    "        Month_Days=[]\n",
    "        Times=[]\n",
    "        year=[]\n",
    "        for i in range(len(date)):\n",
    "            Day_of_week.append(date[i][0])\n",
    "            Month_Days.append(date[i][1]+'/'+date[i][2])\n",
    "            Times.append(date[i][3])\n",
    "            year.append(date[i][4])\n",
    "\n",
    "        # 表格\n",
    "        ptt_parse = pd.DataFrame({'文章標題':title,\n",
    "                                  '推噓':good_bad,\n",
    "                                  '作者':author,\n",
    "                                  '年':year,\n",
    "                                  '月/日':Month_Days,\n",
    "                                  '星期':Day_of_week,\n",
    "                                  '時間':Times,\n",
    "                                  '文章內容':article_content,'文章內容2':article_content_null,\n",
    "                                  '推文內容':push_all,\n",
    "                                  '連結':title_url})\n",
    "\n",
    "        ptt_parse['文章內容'] = ptt_parse['文章內容']+' '+ptt_parse['文章內容2']\n",
    "        del ptt_parse['文章內容2']\n",
    "\n",
    "        \n",
    "\n",
    "        return ptt_parse\n",
    "    \n",
    "    except:\n",
    "        print(f\"{page} error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptt_crawler(1, board='AllPOST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(ptt_crawler(i, board='AllPOST') for i in trange(100,104)).reset_index(drop=True)\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/Users/syunhua/Desktop/000_prase_python/ptt_crawler.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/syunhua/Desktop/ptt_crawler.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "#how to auto turn page\n",
    "def ptt_crawler_2(number):\n",
    "    for i in range(number):\n",
    "        url = 'https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "        response = requests.get(url, cookies={'over18':'1'})\n",
    "        content = response.content.decode()\n",
    "        html = etree.HTML(content)\n",
    "        next_page = html.xpath(\"(//a[@class='btn wide'])[2]/@href\")\n",
    "        next_page = 'https://www.ptt.cc'+next_page[0] \n",
    "        url = next_page\n",
    "        ptt_parse(url=url)"
   ]
  }
 ]
}