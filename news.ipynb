{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, bs4\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import trange\n",
    "import json\n",
    "\n",
    "pd.set_option(\"display.max_columns\",100)\n",
    "pd.set_option(\"display.max_rows\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_crawler(page,board='all'):\n",
    "    news_url = f'https://news.ltn.com.tw/ajax/breakingnews/{board}/{page}'\n",
    "    response = requests.get(news_url)\n",
    "    news_content = response.json()\n",
    "\n",
    "    try:\n",
    "        news_ID=[]\n",
    "        news_type=[]\n",
    "        title=[]\n",
    "        photo_L=[]\n",
    "        urls=[]\n",
    "        news_content_o=[]\n",
    "        for i in news_content['data']:\n",
    "            news_ID.append(i['no'])\n",
    "            news_type.append(i['type_cn'])\n",
    "            title.append(i['title'])\n",
    "            photo_L.append(i['photo_L'])\n",
    "            urls.append(i['url'])\n",
    "            news_content_o.append(i['summary'])\n",
    "    except:\n",
    "        news_ID=[]\n",
    "        news_type=[]\n",
    "        title=[]\n",
    "        photo_L=[]\n",
    "        urls=[]\n",
    "        news_content_o=[]\n",
    "        for i in news_content['data'].values():\n",
    "            news_ID.append(i['no'])\n",
    "            news_type.append(i['type_cn'])\n",
    "            title.append(i['title'])\n",
    "            photo_L.append(i['photo_L'])\n",
    "            urls.append(i['url'])\n",
    "            news_content_o.append(i['summary'])\n",
    "\n",
    "\n",
    "    time_frame=[]\n",
    "    for i in urls:\n",
    "        sub_response = requests.get(i)\n",
    "        sub_content = sub_response.content.decode()\n",
    "        html = etree.HTML(sub_content)\n",
    "        time_frame.append(html.xpath(\"//span[@class='time']/text()\"))\n",
    "\n",
    "    time_n_s=[]\n",
    "    for t in time_frame:\n",
    "        time_n_s.append(t[0])\n",
    "    time_s = [re.sub('\\n', '', time_n_s[i]) for i in range(len(time_n_s))]\n",
    "    time_o = [re.sub(' ', '', time_s[i]) for i in range(len(time_s))]\n",
    "\n",
    "    m=[]\n",
    "    for t in time_o: \n",
    "        m.append(re.findall(r'.{1}',t))\n",
    "    m = pd.DataFrame(m)\n",
    "\n",
    "    year = m[0]+m[1]+m[2]+m[3]\n",
    "    month = m[5]+m[6]\n",
    "    day = m[8]+m[9]\n",
    "    YMD = m[0]+m[1]+m[2]+m[3]+'/'+m[5]+m[6]+'/'+m[8]+m[9]\n",
    "    time = m[10]+m[11]+m[12]+m[13]+m[14]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    news_content = [re.sub('\\n', '', news_content_o[i]) for i in range(len(news_content_o))]\n",
    "\n",
    "    \n",
    "\n",
    "    news_table = pd.DataFrame({'文章代碼':news_ID,\n",
    "                               '類別':news_type,\n",
    "                               '新聞標題':title,\n",
    "                               '日期':YMD,\n",
    "                               '時間':time,\n",
    "                               '文章內容':news_content,\n",
    "                               '新聞圖片':photo_L,\n",
    "                               '連結':urls})\n",
    "    return news_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([news_crawler(i, board='novelty') for i in trange(1,11)]).reset_index(drop=True)\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/Users/syunhua/Desktop/000_prase_python/dpp_news_crawler.csv\")"
   ]
  }
 ]
}